---
title: "Human Activity Recognition Project"
output: html_document
---

###Introduction

The 'quantified self' movement describes a group of enthusiasts who are interested in measuring and recording all aspects of their daily lives using wearable devices such as Jawbone Up, Nike FuelBand, and Fitbit. As a result of this, massive data are generated every day.  One thing these people regularly do is quantifing how much of a particular activity they do, but not on 'how well' they do it.  

###Study design and objectives

Human activity recognition study recuited 6 male participants aged between 20-28 years with little weight lifting experience. They were asked to perform barbell lifting (1.25kg) in 5 different ways,

* Class A: ex:ctly according to the specification (the correct way)
* Class B: throwing the elbows to the front (common error)
* Class C: lifting the dumbbell only halfway (common error)
* Class D: lowering the dumbbell only halfway (common error)
* Class E: throwing the hips to the front (common error)

Data were collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants when they performed weight lifting correctly and incorrectly. 

The objectives of this project are: 

* Predict the manner in which the study subjects did the exercise indicated by classe variable.
* Build a prediction model using features from data set and cross-validation method.
* Estimate the out-of-sample error.
* Use the prediction model to predict 20 seperate test cases.

###Data analysis and results

####Download data
Data sets are downloaded from the course website, "pml-training.csv" data are used for prediction model development; "pml-testing.csv" data contain 20 questions for model testing. 

```{r, echo=TRUE, results='hide'}
library(caret)
library(randomForest)
library(Hmisc)
library(foreach)
library(doParallel)
library(parallel)
```

```{r, echo=TRUE}
url1<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
train.data<- download.file(url1, destfile="/Users/zhihuang/desktop/PML_Project1/pml-training.csv", method="curl")
test.data<- download.file(url2, destfile="/Users/zhihuang/desktop/PML_Project1/pml-testing.csv", method="curl")
```

####Data cleaning

Read in data and convert 'blank' or '#DIV/0!' to NA for the subsequent removal.

```{r, echo=TRUE} 
train<-read.csv("/Users/zhihuang/desktop/PML_Project1/pml-training.csv", na.strings=c("NA","#DIV/0!", ""))
test<-read.csv("/Users/zhihuang/desktop/PML_Project1/pml-testing.csv", na.strings=c("NA","#DIV/0!", ""))
dim(train); dim(test)
```

Remove columns with NA, and also the first 7 columns since they are inrelavent to this analysis.

```{r, echo=TRUE}
train.tidy <-train[, colSums(is.na(train)) == 0][-(1:7)]
test.tidy <-test[, colSums(is.na(test)) == 0][-(1:7)]
dim(train.tidy); dim(test.tidy)
names(train.tidy)
##plot the actual class
plot(train.tidy$classe,col= c("purple","red", "yellow", "blue", "orange"), main = "`Classe` Frequency Plot", xlab="Actual class")
```

####Model building

The class prediction model for this project was fitted using "random forest" method since this method produces the highest accuracy of sample prediction.

 * First we partition the training data into training set and cross-validation set at 6:4 ratio.  We applied 60% of training data to train the model and 40% of the data for testing and estimation of the out-of-sample error.

```{r, echo=TRUE}
set.seed(1000)
inTrain <- createDataPartition(train.tidy$classe, p = 0.6, list = FALSE)
training <- train.tidy[inTrain, ]
testing <- train.tidy[-inTrain, ]
```

* Train the prediction model using the features in the training data set

```{r, echo=TRUE}
registerDoParallel()
classe <- training$classe
variables <- training[-ncol(training)]

model <- foreach(ntree=rep(125, 4), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(variables, classe, ntree=ntree) 
}

```

 * Test the prediction model in cross-validation data set

```{r, echo=TRUE}
training_pred <- predict(model, training)
confusionMatrix(training_pred, training$classe)

testing_pred <- predict(model, testing)
confusionMatrix(testing_pred, testing$classe)

```

```{r, echo=TRUE}
#Normalized confusion matrix in corss validation data set
A <- matrix(c(0.998,0.012,0.000,0.000,0.000,0.002,0.985,0.007,0.000,0.000,0.000,0.003,0.993,0.016,.001,0.000,0.000,0.001,0.984,0.002,0.000,0.000,0.000,0.000,0.997), nrow=5, ncol=5, byrow = TRUE)       
dimnames(A) = list(c("A", "B", "C", "D", "E"),c("A", "B", "C", "D", "E"))   
confusion <- as.data.frame(as.table(A))
plot <- ggplot(confusion)
plot + geom_tile(aes(x=Var1, y=Var2, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + labs(fill="Freq",title = "Normalized Confusion Matrix in Cross-Validataion")
```


The results show that the 'in-sample' accuracy of the model is 100% obtained from the training data; 'the-out-of sample' accuracy is around 99.2% from the cross-validation testing data, the 'out-of-sample error' is <1%. 

####Use the prediction model to predict 20 different testing cases

```{r, echo=TRUE, results='hide'}
answers <- predict(model, test.tidy)
answers <- as.character(answers)
answers

##export answers as txt files
pml_write_files = function(x) {
    n = length(x)
    for (i in 1:n) {
        filename = paste0("problem_id_", i, ".txt")
        write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)
    }
}

pml_write_files(answers)

```

The prediction on 20 test cases are 100% accurate using this model.

####Conclusion

The class prediction model that we built from the trainging data using 'random forest' method yields a highly accurate algorithms (>99%) for test case prediction.  


####References:

(1) Data source: http://groupware.les.inf.puc-rio.br/har.

(2) Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

